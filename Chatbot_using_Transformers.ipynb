{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMLh4El71ZXosF0VorYc0ZM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sumalatha456/MINI_NLP-PROJECTS/blob/main/Chatbot_using_Transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementing chatbot using Transformers"
      ],
      "metadata": {
        "id": "QhEtEQl42liD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import atexit\n",
        "import shutil\n",
        "from transformers import BlenderbotTokenizer, BlenderbotForConditionalGeneration\n",
        "\n",
        "# Step 1: Load the pretrained model\n",
        "model_name = \"facebook/blenderbot-1B-distill\"\n",
        "tokenizer = BlenderbotTokenizer.from_pretrained(model_name)\n",
        "model = BlenderbotForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "# Step 2: Define function for chatbot interaction\n",
        "def interact_with_chatbot(user_input, conversation_history):\n",
        "    # Add user input to the conversation history\n",
        "    conversation_history.append(f\"User: {user_input}\")\n",
        "    conversation_text = \" \".join(conversation_history[-5:])  # Keep last 5 exchanges\n",
        "\n",
        "    # Encode the conversation history\n",
        "    inputs = tokenizer(conversation_text, return_tensors='pt', padding=True, truncation=True)\n",
        "\n",
        "    # Generate response\n",
        "    response_ids = model.generate(**inputs, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n",
        "    response_text = tokenizer.batch_decode(response_ids, skip_special_tokens=True)[0]\n",
        "\n",
        "    # Add model's response to the conversation history\n",
        "    conversation_history.append(f\"Chatbot: {response_text}\")\n",
        "\n",
        "    return response_text\n",
        "\n",
        "# Step 3: Define function to delete model files\n",
        "def delete_model_files():\n",
        "    cache_dir = os.path.expanduser(\"~/.cache/huggingface/hub/models--facebook--blenderbot-1B-distill\")\n",
        "    if os.path.exists(cache_dir):\n",
        "        user_input = input(\"Do you want to delete the model files from the cache directory? (yes/no): \")\n",
        "        if user_input.lower() == \"yes\":\n",
        "            shutil.rmtree(cache_dir)\n",
        "            print(f\"Deleted model files from cache directory: {cache_dir}\")\n",
        "        else:\n",
        "            print(\"Model files will not be deleted from the cache directory.\")\n",
        "    else:\n",
        "        print(f\"Model files do not exist in the cache directory: {cache_dir}\")\n",
        "\n",
        "# Step 4: Register cleanup function to run on exit\n",
        "atexit.register(delete_model_files)\n",
        "\n",
        "# Step 5: Start conversation loop\n",
        "print(\"Welcome to the India Tourism Chatbot!\")\n",
        "print(\"Type 'exit' to end the conversation.\")\n",
        "\n",
        "conversation_history = []\n",
        "\n",
        "while True:\n",
        "    # Step 5.1: Get user input\n",
        "    user_input = input(\"User: \")\n",
        "\n",
        "    # Step 5.2: Check if the user wants to quit\n",
        "    if user_input.lower() == \"exit\":\n",
        "        print(\"Thank you for using the India Tourism Chatbot!\")\n",
        "        break\n",
        "\n",
        "    # Step 5.3: Generate and print chatbot's response\n",
        "    response = interact_with_chatbot(user_input, conversation_history)\n",
        "    print(\"Chatbot:\", response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tqLoehGN_JFl",
        "outputId": "d02ae152-c361-41f9-a120-f60aaf62c91d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to the India Tourism Chatbot!\n",
            "Type 'exit' to end the conversation.\n",
            "User: hello? how are you\n",
            "Chatbot:  Hello, I am doing well. How are you? I hope you are having a good day.\n",
            "User: i am good\n",
            "Chatbot:  I'm doing well, thank you. What do you like to do in your spare time?\n",
            "User: what are the different places there in india?\n",
            "Chatbot:  I like to spend time with my family. I live in the United States. What about you?\n",
            "User: i live in hyderabad.\n",
            "Chatbot:  I love spending time with family as well. I also like to travel to different places in the world.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KU_jiGHD-ueV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}